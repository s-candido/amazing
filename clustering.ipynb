{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f7cfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75df16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "DB_NAME = \"amazing.duckdb\"\n",
    "TABLE_EVENTS = \"all_events\"\n",
    "TABLE_SEGMENTS = \"user_segments\"\n",
    "N_CLUSTERS = 5\n",
    "SAMPLE_USER_PERCENT = 0.005\n",
    "BATCH_SIZE = 1000 \n",
    "\n",
    "con = duckdb.connect(DB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ba0c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement de users avec au moins 10 événements \n",
    "print(\"Chargement d'un échantillon d'utilisateurs actifs...\")\n",
    "\n",
    "user_ids_df = con.execute(f\"\"\"\n",
    "    SELECT user_id\n",
    "    FROM {TABLE_EVENTS}\n",
    "    WHERE user_id IS NOT NULL\n",
    "    GROUP BY user_id\n",
    "    HAVING COUNT(*) >= 10\n",
    "\"\"\").fetch_df()\n",
    "\n",
    "sampled_user_ids = user_ids_df.sample(frac=SAMPLE_USER_PERCENT, random_state=42)['user_id'].tolist()\n",
    "\n",
    "print(f\"Nombre d'utilisateurs actifs échantillonnés : {len(sampled_user_ids)}\")\n",
    "\n",
    "#  Création des features utilisateurs batch par batch \n",
    "print(\"Création des features utilisateurs par batch...\")\n",
    "\n",
    "user_features_list = []\n",
    "\n",
    "for i in tqdm(range(0, len(sampled_user_ids), BATCH_SIZE), desc=\"Avancement user features\", ncols=100):\n",
    "    batch_ids = sampled_user_ids[i:i+BATCH_SIZE]\n",
    "    batch_ids_str = \",\".join(f\"'{uid}'\" for uid in batch_ids)\n",
    "\n",
    "    batch_query = f\"\"\"\n",
    "    WITH\n",
    "        base_events AS (\n",
    "            SELECT\n",
    "                user_id,\n",
    "                event_type,\n",
    "                event_time,\n",
    "                price,\n",
    "                LEAD(event_time) OVER (PARTITION BY user_id ORDER BY event_time) AS next_event_time\n",
    "            FROM {TABLE_EVENTS}\n",
    "            WHERE user_id IN ({batch_ids_str})\n",
    "        ),\n",
    "        features AS (\n",
    "            SELECT\n",
    "                user_id,\n",
    "                COUNT(*) AS total_events,\n",
    "                SUM(CASE WHEN event_type = 'view' THEN 1 ELSE 0 END) AS total_views,\n",
    "                SUM(CASE WHEN event_type = 'purchase' THEN 1 ELSE 0 END) AS total_purchases,\n",
    "                AVG(EXTRACT(EPOCH FROM (next_event_time - event_time))) AS avg_time_between_events,\n",
    "                SUM(CASE WHEN event_type = 'purchase' THEN price ELSE 0 END) AS total_spent,\n",
    "                COALESCE(AVG(CASE WHEN event_type = 'purchase' THEN price ELSE NULL END), 0) AS avg_basket,\n",
    "                MAX(event_time) AS last_event_time\n",
    "            FROM base_events\n",
    "            GROUP BY user_id\n",
    "    )\n",
    "    SELECT\n",
    "        *,\n",
    "        CASE WHEN total_views > 0 THEN total_purchases * 1.0 / total_views ELSE 0 END AS conversion_rate,\n",
    "        CASE WHEN (total_views + total_purchases) > 0 THEN total_purchases * 1.0 / (total_views + total_purchases) ELSE 0 END AS purchase_ratio,\n",
    "        DATE_PART('day', CURRENT_TIMESTAMP - last_event_time) AS days_since_last_event\n",
    "    FROM features\n",
    "    \"\"\"\n",
    "\n",
    "    batch_features = con.execute(batch_query).fetch_df()\n",
    "\n",
    "    # Récupérer les user_id de cette batch\n",
    "    valid_user_ids = con.execute(f\"\"\"\n",
    "        SELECT user_id\n",
    "        FROM {TABLE_EVENTS}\n",
    "        WHERE user_id IN ({batch_ids_str})\n",
    "        GROUP BY user_id\n",
    "        HAVING COUNT(*) >= 10\n",
    "    \"\"\").fetch_df()\n",
    "    valid_user_ids = set(valid_user_ids[\"user_id\"].astype(str))\n",
    "\n",
    "    # Filtrage strict des user_id valides\n",
    "    batch_features = batch_features[batch_features[\"user_id\"].astype(str).isin(valid_user_ids)]\n",
    "\n",
    "    user_features_list.append(batch_features)\n",
    "\n",
    "# Fusionner tous les batchs\n",
    "user_features = pd.concat(user_features_list, ignore_index=True)\n",
    "\n",
    "# Vérification des NaN\n",
    "print(\"Vérification des NaN\")\n",
    "nan_summary = user_features.isna().sum()\n",
    "print(\"Résumé des NaN par colonne :\")\n",
    "print(nan_summary[nan_summary > 0])\n",
    "\n",
    "users_with_nan = user_features[user_features.isna().any(axis=1)]\n",
    "print(f\"Nombre d'utilisateurs avec des NaN : {len(users_with_nan)}\")\n",
    "print(\"Exemples d'utilisateurs avec NaN :\")\n",
    "print(users_with_nan.head(10))\n",
    "\n",
    "# Standardisation\n",
    "print(\"Standardisation des features...\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(user_features.drop(columns=[\"last_event_time\"]))\n",
    "\n",
    "# Clustering\n",
    "print(\"Clustering avec KMeans...\")\n",
    "kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "user_features[\"segment\"] = clusters\n",
    "\n",
    "#  Sauvegarde des résultats dans DuckDB\n",
    "print(f\"Sauvegarde dans {TABLE_SEGMENTS}...\")\n",
    "con.execute(f\"DROP TABLE IF EXISTS {TABLE_SEGMENTS}\")\n",
    "con.register(\"temp_user_features\", user_features)\n",
    "con.execute(f\"CREATE TABLE {TABLE_SEGMENTS} AS SELECT * FROM temp_user_features\")\n",
    "\n",
    "\n",
    "print(\"Clustering sauvegarder avec succès !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f023e251",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Visualisation du clustering ---\n",
    "print(\"Visualisation du clustering...\")\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=user_features[\"segment\"], cmap='tab10', s=10)\n",
    "plt.title(\"Visualisation des clusters utilisateurs (PCA)\")\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbb4fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lister toutes les tables et colonnes des tables\n",
    "print(\"Tables disponibles dans la base :\")\n",
    "tables = con.execute(\"SHOW TABLES\").fetch_df()\n",
    "print(tables)\n",
    "\n",
    "print(\"Structure de la table all_events :\")\n",
    "schema_all_events = con.execute(\"DESCRIBE TABLE all_events\").fetch_df()\n",
    "print(schema_all_events)\n",
    "\n",
    "print(\"Structure de la table user_segments :\")\n",
    "schema_user_segments = con.execute(\"DESCRIBE TABLE user_segments\").fetch_df()\n",
    "print(schema_user_segments)\n",
    "\n",
    "df_purchase = con.execute(\"SELECT * FROM user_segments LIMIT 10\").fetch_df()\n",
    "print(df_purchase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a22c898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fermeture de la connexion DuckDB\n",
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
